# -*- coding: utf-8 -*-
"""Image Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMUUCsMVY5V1awM13bqAvk-_JdYdi4R2
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from pycocotools.coco import COCO
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json

from google.colab import drive
drive.mount('/content/drive')

train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'

train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300'

with open(train_annotation_file) as f:
    train_coco_data = json.load(f)

print(f"Total training images: {len(train_coco_data['images'])}")
print(f"Total training annotations: {len(train_coco_data['annotations'])}")
print(f"Total categories: {len(train_coco_data['categories'])}")

import json

train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'

train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/data'  # added /data to keep same structure?

# Load training annotation JSON
with open(train_annotation_file) as f:
    train_coco_data = json.load(f)

# Load validation annotation JSON
with open(val_annotation_file) as f:
    val_coco_data = json.load(f)

print(f"Total training images: {len(train_coco_data['images'])}")
print(f"Total training annotations: {len(train_coco_data['annotations'])}")
print(f"Total training categories: {len(train_coco_data['categories'])}")

print(f"Total validation images: {len(val_coco_data['images'])}")
print(f"Total validation annotations: {len(val_coco_data['annotations'])}")
print(f"Total validation categories: {len(val_coco_data['categories'])}")

from pycocotools.coco import COCO

# Update this path if needed
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'

# Create the COCO object
coco_train = COCO(train_annotation_file)

all_cats = coco_train.loadCats(coco_train.getCatIds())
print("All categories in dataset:")
for c in all_cats:
    print(c['id'], c['name'])

# Step 1: Define your target classes
target_classes = ['cake', 'car', 'dog', 'person']

# Step 2: Get their category IDs from COCO
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

# Step 3: Print the result
print("Target category IDs:", target_cat_ids)

target_classes = ['cake', 'car', 'dog', 'person']

all_cats = coco_train.loadCats(coco_train.getCatIds())
all_cat_names = [c['name'] for c in all_cats]
print("Dataset categories:", all_cat_names)

# Find closest matches ignoring case
for target in target_classes:
    matches = [c['name'] for c in all_cats if c['name'].lower() == target.lower()]
    print(f"Looking for '{target}' found matches: {matches}")

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)
# Output should be: [15, 16, 25, 41]

anns = coco_train.loadAnns(coco_train.getAnnIds(catIds=target_cat_ids))
print(f"Number of annotations for target categories: {len(anns)}")

img_ids = coco_train.getImgIds(catIds=target_cat_ids)
print(f"Number of images containing target categories: {len(img_ids)}")

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)

image_ids = set(ann['image_id'] for ann in anns)
print(f"Number of images containing target categories (manual): {len(image_ids)}")

for i in range(10):
    ann = coco_train.dataset['annotations'][i]
    cat_id = ann['category_id']
    cat_name = next(cat['name'] for cat in coco_train.dataset['categories'] if cat['id'] == cat_id)
    print(f"Annotation {i} category: {cat_name}")

import matplotlib.pyplot as plt
import random
import cv2

def show_random_samples(coco, image_ids, img_dir, n=3):
    selected_ids = random.sample(image_ids, min(n, len(image_ids)))

    for img_id in selected_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = coco.getAnnIds(imgIds=img_id)
        anns = coco.loadAnns(ann_ids)

        # Draw masks or bounding boxes
        for ann in anns:
            bbox = ann['bbox']
            x, y, w, h = map(int, bbox)
            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
            cat_name = coco.loadCats(ann['category_id'])[0]['name']
            cv2.putText(image, cat_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)

        plt.figure(figsize=(8, 6))
        plt.imshow(image)
        plt.title(f"Image ID: {img_id}")
        plt.axis('off')
        plt.show()
show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)

# Check number of annotations for target categories
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
print(f"Total annotations for target categories: {len(ann_ids)}")

# Load those annotations
anns = coco_train.loadAnns(ann_ids)

# Collect unique image IDs from these annotations
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))
print(f"Number of unique images with target annotations: {len(image_ids_with_targets)}")

# If you have images, let's try visualizing from this list instead
if len(image_ids_with_targets) > 0:
    # Use your visualization function with image_ids_with_targets instead of img_ids
    show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)
else:
    print("No images found with the target categories' annotations.")

image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# === Step 1: Paths and COCO setup ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

# Your target classes exactly as per dataset categories
target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)

# Get all annotations for target categories
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
print(f"Total annotations for target categories: {len(ann_ids)}")

anns = coco_train.loadAnns(ann_ids)

# Collect unique image IDs with these annotations
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))
print(f"Number of unique images with target annotations: {len(image_ids_with_targets)}")

# === Step 2: Define Albumentations transform ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 3: Custom Dataset ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 4: Create dataset and dataloader ===
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# === Step 5: Visualize a batch ===
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)  # unnormalize
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# Load one batch and visualize
images, masks = next(iter(loader))
visualize_batch(images, masks)

!ls "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300"

train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/annotations.json'

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
import torch.nn as nn
import torchvision.models as models
import torch.nn.functional as F

# === Step 1: Paths and COCO setup ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
print(f"Total annotations for target categories: {len(ann_ids)}")

anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))
print(f"Number of unique images with target annotations: {len(image_ids_with_targets)}")

# === Step 2: Albumentations transforms ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 3: Custom Dataset ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 4: Dataloader ===
def collate_fn(batch):
    return tuple(zip(*batch))

dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# === Step 5: Model ===
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super(SimpleSegmentationModel, self).__init__()
        self.backbone = models.resnet18(weights="IMAGENET1K_V1")
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
        self.up = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, num_classes, kernel_size=1)
        )

    def forward(self, x):
        x = self.backbone(x)
        x = self.up(x)
        return x

# === Step 6: Training Setup ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(target_classes) + 1
model = SimpleSegmentationModel(num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# === Step 7: Training Loop ===
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for images, masks in loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device)

        outputs = model(images)
        outputs = F.interpolate(outputs, size=masks.shape[1:], mode="bilinear", align_corners=False)

        loss = criterion(outputs, masks.long())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(loader):.4f}")

# === Step 8: Save Model ===
torch.save(model.state_dict(), "segmentation_model.pth")

# === Step 9: Visualize Predictions ===
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1, 2, 0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) +
                      np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# Predict on one batch
model.eval()
with torch.no_grad():
    for images, masks in loader:
        images = torch.stack(images).to(device)
        outputs = model(images)
        preds = torch.argmax(outputs, dim=1)
        visualize_batch(images.cpu(), preds.cpu())
        break

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
import torch.nn as nn
import torchvision.models as models

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# === Step 1: Setup COCO dataset and paths ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

print(f"Total images with target classes: {len(image_ids_with_targets)}")
print(f"Total annotations: {len(anns)}")

# === Step 2: Define transforms ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 3: Dataset class ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 4: Model definition with fixed decoder ===
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.encoder = nn.Sequential(*list(backbone.children())[:-2])

        self.decoder = nn.Sequential(
            nn.Conv2d(2048, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/32 -> 1/16
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/16 -> 1/8
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/8 -> 1/4
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/4 -> 1/2
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/2 -> 1/1 (full res)
            nn.Conv2d(32, num_classes, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# === Step 5: Create dataset and dataloader ===
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

num_classes = len(target_classes) + 1  # background class included
model = SimpleSegmentationModel(num_classes=num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# === Step 6: Training loop with visualization ===
def visualize_batch(images, masks, preds=None):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(15,5))
        plt.subplot(1,3,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,3,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Ground Truth Mask")
        plt.axis('off')

        if preds is not None:
            pred_mask = preds[i].cpu().numpy()
            plt.subplot(1,3,3)
            plt.imshow(pred_mask, cmap='jet', alpha=0.5)
            plt.title("Predicted Mask")
            plt.axis('off')

        plt.show()

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for images, masks in loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device).long()  # <-- FIX here: cast masks to long

        optimizer.zero_grad()
        outputs = model(images)  # (B, C, H, W)

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.4f}")

    # Visualize predictions on a batch
    model.eval()
    with torch.no_grad():
        sample_images, sample_masks = next(iter(loader))
        sample_images_tensor = torch.stack(sample_images).to(device)
        preds = model(sample_images_tensor)
        preds = torch.argmax(preds, dim=1)  # (B,H,W)
        visualize_batch(sample_images, sample_masks, preds=preds)
    model.train()

import matplotlib.pyplot as plt
import random
import cv2

def show_random_samples(coco, image_ids, img_dir, n=3):
    selected_ids = random.sample(image_ids, min(n, len(image_ids)))

    for img_id in selected_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = coco.getAnnIds(imgIds=img_id)
        anns = coco.loadAnns(ann_ids)

        # Draw masks or bounding boxes
        for ann in anns:
            bbox = ann['bbox']
            x, y, w, h = map(int, bbox)
            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
            cat_name = coco.loadCats(ann['category_id'])[0]['name']
            cv2.putText(image, cat_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)

        plt.figure(figsize=(8, 6))
        plt.imshow(image)
        plt.title(f"Image ID: {img_id}")
        plt.axis('off')
        plt.show()
show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)

# --- Imports ---
import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# --- Step 1: Setup paths and load COCO annotations ---
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

# Target classes
target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

# Get all annotation IDs for target classes
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)

# Unique image IDs containing target annotations
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

print(f"Total target annotations: {len(ann_ids)}")
print(f"Unique images with targets: {len(image_ids_with_targets)}")

# --- Step 2: Define data augmentation + preprocessing ---
def get_transform(train=True):
    if train:
        return A.Compose([
            A.Resize(256, 256),
            A.HorizontalFlip(p=0.5),
            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(256, 256),
            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
            ToTensorV2()
        ])

# --- Step 3: Custom Dataset ---
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0 = background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            label = self.cat_id_to_label[ann['category_id']]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.tensor(image).permute(2,0,1).float() / 255.0
            mask = torch.tensor(mask).long()

        return image, mask

# --- Step 4: Create dataset and dataloader ---
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform(train=True)
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# --- Step 5: Visualize a batch ---
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# Run visualization
images, masks = next(iter(loader))
visualize_batch(images, masks)

import os
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision.models.segmentation.deeplabv3 import DeepLabHead
import torch.nn as nn
import torch.optim as optim
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# --- Paths ---
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

# --- Load COCO annotations ---
coco_train = COCO(train_annotation_file)
target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

print(f"Total target annotations: {len(ann_ids)}")
print(f"Unique images with targets: {len(image_ids_with_targets)}")

# --- Albumentations Transform ---
def get_transform(train=True):
    if train:
        return A.Compose([
            A.Resize(256, 256),
            A.HorizontalFlip(p=0.5),
            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(256, 256),
            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
            ToTensorV2()
        ])

# --- Custom Dataset ---
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms
        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            label = self.cat_id_to_label[ann['category_id']]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask'].long()  # <--- FIX: cast mask to long here!
        else:
            image = torch.tensor(image).permute(2,0,1).float() / 255.0
            mask = torch.tensor(mask).long()

        return image, mask

# --- Dataset and DataLoader ---
dataset = CustomCocoDataset(coco_train, image_ids_with_targets, train_img_dir, target_classes, get_transform(train=True))

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# --- Model Setup ---
num_classes = len(target_classes) + 1  # background + classes
model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)
model.classifier = DeepLabHead(2048, num_classes)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# --- Training Loop ---
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, masks in loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device)

        optimizer.zero_grad()
        outputs = model(images)['out']

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(loader):.4f}")

# --- Save model ---
torch.save(model.state_dict(), 'segmentation_model.pth')

# --- Visualization ---
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# --- Visualize predictions ---
model.eval()
with torch.no_grad():
    images, masks = next(iter(loader))
    images_tensor = torch.stack(images).to(device)
    outputs = model(images_tensor)['out']
    preds = torch.argmax(outputs, dim=1).cpu()

visualize_batch(images, preds)

import os
import random
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torchmetrics
import matplotlib.pyplot as plt

# === Paths and COCO setup ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

# === Albumentations transforms ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Custom Dataset with mask.long() fix ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0 = background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask'].long()  # <-- important fix here
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Dataloader ===
def collate_fn(batch):
    return tuple(zip(*batch))

dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)
loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# === Simple model example ===
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU(),
            nn.ConvTranspose2d(32, num_classes, 2, stride=2)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_classes = len(target_classes) + 1  # background included
model = SimpleSegmentationModel(num_classes).to(device)

# === Loss, optimizer, metric ===
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
miou_metric = torchmetrics.JaccardIndex(task="multiclass", num_classes=num_classes).to(device)

# === Training loop snippet ===
model.train()
for images, masks in loader:
    images = torch.stack(images).to(device)
    masks = torch.stack(masks).to(device)

    optimizer.zero_grad()
    outputs = model(images)  # (B, C, H, W)

    loss = criterion(outputs, masks)
    loss.backward()
    optimizer.step()

    preds = torch.argmax(outputs, dim=1)
    miou = miou_metric(preds, masks)
    print(f"Loss: {loss.item():.4f} - mIoU: {miou.item():.4f}")

# === Visualization function (optional) ===
def visualize_batch(images, masks):
    for i in range(len(images)):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406]), 0,1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.axis('off')
        plt.title("Image")

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.axis('off')
        plt.title("Mask")

        plt.show()

images, masks = next(iter(loader))
visualize_batch(images, masks)

import os
from pycocotools.coco import COCO

def verify_image_paths(coco_annotation_file, image_dir):
    coco = COCO(coco_annotation_file)
    image_ids = coco.getImgIds()
    missing_images = []

    for img_id in image_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(image_dir, img_info['file_name'])
        if not os.path.exists(img_path):
            missing_images.append(img_path)

    if len(missing_images) == 0:
        print("✅ All images exist!")
    else:
        print(f"❌ Missing {len(missing_images)} images:")
        for p in missing_images:
            print(p)

# Example usage for your train set:
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

verify_image_paths(train_annotation_file, train_img_dir)

import os
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# --- Dataset class ---
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms
        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()))
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask'].long()  # <-- fix: convert mask to long here
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# --- Transform ---
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406),
                    std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# --- Paths and COCO setup ---
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)
target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

# --- Dataset and DataLoader ---
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# --- Simple segmentation model ---
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, num_classes, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# --- Setup ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_classes = len(target_classes) + 1  # include background

model = SimpleSegmentationModel(num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# --- Training example: 1 epoch ---
model.train()
for images, masks in loader:
    images = torch.stack(images).to(device)
    masks = torch.stack(masks).to(device)

    optimizer.zero_grad()
    outputs = model(images)  # [B, num_classes, H, W]

    loss = criterion(outputs, masks)
    loss.backward()
    optimizer.step()

    print(f"Loss: {loss.item():.4f}")

# --- Visualization function ---
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')
        plt.show()

# Visualize one batch
images, masks = next(iter(loader))
visualize_batch(images, masks)

import torchmetrics

# --- Setup metric ---
miou_metric = torchmetrics.JaccardIndex(num_classes=num_classes, task="multiclass").to(device)

# --- Validation function ---
def validate(model, val_loader, criterion, metric, device):
    model.eval()
    val_loss = 0
    metric.reset()

    with torch.no_grad():
        for images, masks in val_loader:
            images = torch.stack(images).to(device)
            masks = torch.stack(masks).to(device)

            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            metric.update(preds, masks)

    val_loss /= len(val_loader.dataset)
    val_miou = metric.compute().item()
    return val_loss, val_miou

# --- Training function for one epoch ---
def train_one_epoch(model, train_loader, criterion, optimizer, metric, device):
    model.train()
    train_loss = 0
    metric.reset()

    for images, masks in train_loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)
        preds = torch.argmax(outputs, dim=1)
        metric.update(preds, masks)

    train_loss /= len(train_loader.dataset)
    train_miou = metric.compute().item()
    return train_loss, train_miou

# --- Create validation DataLoader (use same classes and transforms but no augmentation) ---
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/data'

coco_val = COCO(val_annotation_file)
val_ann_ids = coco_val.getAnnIds(catIds=target_cat_ids)
val_anns = coco_val.loadAnns(val_ann_ids)
val_image_ids = list(set([ann['image_id'] for ann in val_anns]))

val_dataset = CustomCocoDataset(
    coco=coco_val,
    image_ids=val_image_ids,
    img_dir=val_img_dir,
    classes=target_classes,
    transforms=A.Compose([
        A.Resize(256, 256),
        A.Normalize(mean=(0.485, 0.456, 0.406),
                    std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])
)

val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

# --- Run full training with validation ---
num_epochs = 10

for epoch in range(num_epochs):
    train_loss, train_miou = train_one_epoch(model, loader, criterion, optimizer, miou_metric, device)
    val_loss, val_miou = validate(model, val_loader, criterion, miou_metric, device)

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  Train Loss: {train_loss:.4f} | Train mIoU: {train_miou:.4f}")
    print(f"  Val Loss:   {val_loss:.4f} | Val mIoU:   {val_miou:.4f}")

import torch
import torch.nn as nn
import torch.optim as optim
import torchmetrics
from tqdm import tqdm

# --- Simple example segmentation model ---
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2), nn.ReLU(),
            nn.Conv2d(64, num_classes, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# --- Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(target_classes) + 1  # include background

model = SimpleSegmentationModel(num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
miou_metric = torchmetrics.JaccardIndex(num_classes=num_classes, task="multiclass").to(device)

# --- Training function ---
def train_one_epoch(model, loader, criterion, optimizer, metric, device):
    model.train()
    running_loss = 0.0
    metric.reset()
    for images, masks in tqdm(loader):
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device).long()

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        preds = outputs.argmax(dim=1)
        metric.update(preds, masks)

    avg_loss = running_loss / len(loader) if len(loader) > 0 else 0
    avg_miou = metric.compute().item() if len(loader) > 0 else 0
    return avg_loss, avg_miou

# --- Validation function with empty check ---
def validate(model, loader, criterion, metric, device):
    model.eval()
    running_loss = 0.0
    metric.reset()
    num_batches = len(loader)
    if num_batches == 0:
        print("Warning: Validation loader is empty!")
        return None, None

    with torch.no_grad():
        for images, masks in loader:
            images = torch.stack(images).to(device)
            masks = torch.stack(masks).to(device).long()

            outputs = model(images)
            loss = criterion(outputs, masks)

            running_loss += loss.item()
            preds = outputs.argmax(dim=1)
            metric.update(preds, masks)

    avg_loss = running_loss / num_batches
    avg_miou = metric.compute().item()
    return avg_loss, avg_miou

# --- Run training ---
num_epochs = 10
best_val_miou = 0

print(f"Validation dataset size: {len(val_loader.dataset)}")

for epoch in range(num_epochs):
    train_loss, train_miou = train_one_epoch(model, train_loader, criterion, optimizer, miou_metric, device)
    val_loss, val_miou = validate(model, val_loader, criterion, miou_metric, device)
    if val_loss is None:
        print("Skipping validation metrics due to empty validation set.")
        val_loss, val_miou = 0, 0
    print(f"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train mIoU: {train_miou:.4f} | Val Loss: {val_loss:.4f} | Val mIoU: {val_miou:.4f}")

    if val_miou > best_val_miou:
        best_val_miou = val_miou
        torch.save(model.state_dict(), "/content/best_segmentation_model.pth")
        print("Saved best model.")

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# === Step 1: Define paths and target classes ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/data'  # Adjust if needed

target_classes = ['cake', 'car', 'dog', 'person']

# === Step 2: Load COCO annotations ===
coco_train = COCO(train_annotation_file)
coco_val = COCO(val_annotation_file)

# === Step 3: Get target category IDs ===
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)

# === Step 4: Collect all train image IDs containing target classes ===
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
train_image_ids = list(set([ann['image_id'] for ann in anns]))
print(f"Training images with target classes: {len(train_image_ids)}")

# === Step 5: Filter valid image IDs for validation (skip missing files) ===
def filter_valid_image_ids(coco, img_dir):
    all_img_ids = coco.getImgIds()
    valid_img_ids = []
    missing_files = []

    for img_id in all_img_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        if os.path.isfile(img_path):
            valid_img_ids.append(img_id)
        else:
            missing_files.append(img_info['file_name'])

    print(f"Valid images found: {len(valid_img_ids)}. Missing: {len(missing_files)}")
    if missing_files:
        print("First 10 missing validation images:", missing_files[:10])
    return valid_img_ids

val_image_ids = filter_valid_image_ids(coco_val, val_img_dir)

# === Step 6: Define Albumentations transforms ===
def get_transform(train=True):
    if train:
        return A.Compose([
            A.Resize(256, 256),
            A.HorizontalFlip(p=0.5),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(256, 256),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])

# === Step 7: Create Custom Dataset ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0 = background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 8: Create datasets and loaders ===
train_dataset = CustomCocoDataset(coco_train, train_image_ids, train_img_dir, target_classes, transforms=get_transform(train=True))
val_dataset = CustomCocoDataset(coco_val, val_image_ids, val_img_dir, target_classes, transforms=get_transform(train=False))

def collate_fn(batch):
    return tuple(zip(*batch))

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

# === Step 9: Visualize one batch to confirm data ===
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)  # unnormalize
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# Visualize batch
images, masks = next(iter(train_loader))
visualize_batch(images, masks)

# === Step 10: Model, loss, optimizer setup example (add your model here) ===
import torch.nn as nn
import torch.optim as optim
import torchvision.models.segmentation as segmentation_models

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

num_classes = len(target_classes) + 1  # +1 for background

model = segmentation_models.fcn_resnet50(pretrained=False, num_classes=num_classes)
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# === Step 11: Training and validation loop (simplified example) ===
from tqdm import tqdm

def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, masks in tqdm(loader):
        images = list(img.to(device) for img in images)
        masks = list(mask.to(device) for mask in masks)

        # Stack batch tensors
        images = torch.stack(images)
        masks = torch.stack(masks).long()  # Ensure long type for CrossEntropyLoss

        optimizer.zero_grad()
        outputs = model(images)['out']
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(loader)

def validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    with torch.no_grad():
        for images, masks in loader:
            images = list(img.to(device) for img in images)
            masks = list(mask.to(device) for mask in masks)

            images = torch.stack(images)
            masks = torch.stack(masks).long()

            outputs = model(images)['out']
            loss = criterion(outputs, masks)
            running_loss += loss.item()
    return running_loss / len(loader)

# Example training for 5 epochs
num_epochs = 5
for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_loss = validate(model, val_loader, criterion, device)
    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}")

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# === Step 1: Paths and COCO setup ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300'

coco_train = COCO(train_annotation_file)
coco_val = COCO(val_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

# Get image IDs with target categories
train_ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
train_anns = coco_train.loadAnns(train_ann_ids)
train_img_ids = list(set([ann['image_id'] for ann in train_anns]))

val_ann_ids = coco_val.getAnnIds(catIds=target_cat_ids)
val_anns = coco_val.loadAnns(val_ann_ids)
val_img_ids = list(set([ann['image_id'] for ann in val_anns]))

# === Step 2: Filter image IDs to only those images that exist on disk ===
def filter_existing_images(coco, img_dir, img_ids):
    existing_img_ids = []
    for img_id in img_ids:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(img_dir, img_info['file_name'])
        if os.path.isfile(img_path):
            existing_img_ids.append(img_id)
        else:
            print(f"Warning: Missing image file: {img_path}")
    return existing_img_ids

train_img_ids = filter_existing_images(coco_train, train_img_dir, train_img_ids)
val_img_ids = filter_existing_images(coco_val, val_img_dir, val_img_ids)

# === Step 3: Albumentations transform ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 4: Custom Dataset ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at path: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 5: Create datasets and dataloaders ===
train_dataset = CustomCocoDataset(coco_train, train_img_ids, train_img_dir, target_classes, transforms=get_transform())
val_dataset = CustomCocoDataset(coco_val, val_img_ids, val_img_dir, target_classes, transforms=get_transform())

def collate_fn(batch):
    return tuple(zip(*batch))

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

# === Step 6: Visualize batch function ===
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)  # unnormalize
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# === Step 7: Visualize a batch ===
images, masks = next(iter(train_loader))
visualize_batch(images, masks)

# Now you can continue with model definition, training, validation, etc.

import os
import torch
import torchvision
from torchvision.models.detection.mask_rcnn import maskrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from pycocotools.coco import COCO
import numpy as np
import cv2
from torch.utils.data import Dataset, DataLoader

# --- 1. Dataset Class for Mask R-CNN ---
class CocoInstanceSegmentationDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, cat_ids, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.cat_ids = cat_ids
        self.transforms = transforms

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids, iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        masks = []
        boxes = []
        labels = []

        for ann in anns:
            rle = self.coco.annToRLE(ann)
            mask = self.coco.decodeMask(rle) if hasattr(self.coco, 'decodeMask') else self.coco.annToMask(ann)
            masks.append(mask)

            bbox = ann['bbox']
            # COCO bbox is [x,y,width,height], convert to [xmin, ymin, xmax, ymax]
            boxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])

            labels.append(self.cat_ids.index(ann['category_id']) + 1)  # labels start at 1 (0=background)

        if len(masks) > 0:
            masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
        else:
            # No annotations for this image - create empty tensors
            masks = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)

        target = {}
        target['boxes'] = boxes
        target['labels'] = labels
        target['masks'] = masks
        target['image_id'] = torch.tensor([img_id])

        image = F.to_tensor(image)  # converts to float tensor and normalizes [0,1]

        if self.transforms:
            image, target = self.transforms(image, target)

        return image, target

# --- 2. Setup COCO and DataLoaders ---
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
image_ids_with_targets = list(set(id for ann in coco_train.loadAnns(coco_train.getAnnIds(catIds=target_cat_ids)) for id in [ann['image_id']]))

train_dataset = CocoInstanceSegmentationDataset(coco_train, image_ids_with_targets, train_img_dir, target_cat_ids)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

# --- 3. Load Mask R-CNN with correct number of classes ---
num_classes = 1 + len(target_classes)  # background + classes

model = maskrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)
model.load_state_dict(torch.load('/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth', map_location='cpu'))
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

# --- 4. Evaluation helper ---
def evaluate(model, dataloader, device):
    model.eval()
    with torch.no_grad():
        for images, targets in dataloader:
            images = list(img.to(device) for img in images)
            outputs = model(images)
            # outputs is list of dicts with keys: boxes, labels, scores, masks
            for i in range(len(images)):
                img = images[i].cpu().permute(1, 2, 0).numpy()
                pred_masks = outputs[i]['masks'].cpu().numpy() > 0.5
                # visualize or compute metrics here as needed
                print(f"Image {i}: Predicted {len(pred_masks)} masks")

# --- 5. Run evaluation ---
evaluate(model, train_loader, device)

import os
import torch
import torchvision
from torchvision.models.detection.mask_rcnn import maskrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from pycocotools.coco import COCO
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
import time

# --- Dataset class ---
class CocoInstanceSegmentationDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, cat_ids):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.cat_ids = cat_ids

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image {img_path} not found")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids, iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        masks = []
        boxes = []
        labels = []

        for ann in anns:
            mask = self.coco.annToMask(ann)
            masks.append(mask)
            bbox = ann['bbox']
            boxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])
            labels.append(self.cat_ids.index(ann['category_id']) + 1)  # 0 is background

        if len(masks) > 0:
            masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
        else:
            masks = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)

        target = {
            'masks': masks,
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([img_id])
        }

        image = F.to_tensor(image)

        return image, target

# --- Setup ---
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
image_ids_with_targets = list(set(ann['image_id'] for ann in coco_train.loadAnns(coco_train.getAnnIds(catIds=target_cat_ids))))

train_dataset = CocoInstanceSegmentationDataset(coco_train, image_ids_with_targets, train_img_dir, target_cat_ids)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

# --- Model ---
num_classes = 1 + len(target_classes)  # background + classes
model = maskrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Load saved weights
model.load_state_dict(torch.load('/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth', map_location=device))

# --- Training setup ---
import torch.optim as optim
optimizer = optim.Adam(model.parameters(), lr=1e-4)
num_epochs = 5

# --- Training loop ---
model.train()
for epoch in range(num_epochs):
    epoch_loss = 0
    start_time = time.time()
    for images, targets in train_loader:
        images = list(img.to(device) for img in images)
        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        epoch_loss += losses.item()

    print(f"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss/len(train_loader):.4f} Time: {time.time()-start_time:.1f}s")

# --- Evaluation (simple example) ---
model.eval()
with torch.no_grad():
    for images, targets in train_loader:
        images = list(img.to(device) for img in images)
        outputs = model(images)

        for i, output in enumerate(outputs):
            print(f"Image {i} predicted {len(output['masks'])} masks")

        break  # just one batch for demo

import pandas as pd

image_sizes = [(img['width'], img['height']) for img in coco.dataset['images']]
df_sizes = pd.DataFrame(image_sizes, columns=['Width', 'Height'])

plt.figure(figsize=(6, 6))
sns.histplot(df_sizes, bins=20, kde=True)
plt.title("Image Width and Height Distribution")
plt.show()

areas = [ann['area'] for ann in coco.dataset['annotations'] if ann['category_id'] in cat_ids]

plt.figure(figsize=(8, 5))
sns.histplot(areas, bins=30, log_scale=True, color='salmon')
plt.title("Annotation Area Distribution (log scale)")
plt.xlabel("Area (pixels)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6, 6))
plt.pie(cat_counts, labels=cat_names, autopct='%1.1f%%', startangle=140, colors=sns.color_palette("pastel"))
plt.title("Distribution of Annotations by Category")
plt.axis('equal')
plt.show()

from collections import defaultdict

cat_img_freq = defaultdict(set)
for ann in coco.dataset['annotations']:
    if ann['category_id'] in cat_ids:
        cat_img_freq[ann['category_id']].add(ann['image_id'])

cat_names = [coco.loadCats([cid])[0]['name'] for cid in cat_ids]
img_freq = [len(cat_img_freq[cid]) for cid in cat_ids]

plt.figure(figsize=(8, 5))
sns.barplot(x=cat_names, y=img_freq, palette='flare')
plt.title("Number of Images Containing Each Category")
plt.xlabel("Category")
plt.ylabel("Image Count")
plt.tight_layout()
plt.show()

avg_area_per_category = {}
for cat_id in cat_ids:
    ann_ids = coco.getAnnIds(catIds=[cat_id])
    areas = [coco.loadAnns([ann_id])[0]['area'] for ann_id in ann_ids]
    avg_area_per_category[cat_id] = np.mean(areas)

cat_names = [coco.loadCats([cid])[0]['name'] for cid in avg_area_per_category.keys()]
avg_areas = list(avg_area_per_category.values())

plt.figure(figsize=(8, 5))
sns.barplot(x=cat_names, y=avg_areas, palette='crest')
plt.title("Average Annotation Area per Category")
plt.xlabel("Category")
plt.ylabel("Average Area (pixels)")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you already have:
# cat_names = list of category names
# cat_counts = number of annotations per category

plt.figure(figsize=(10,6))
sns.barplot(x=cat_names, y=cat_counts, palette='viridis')
plt.title("Number of Annotations per Category")
plt.xlabel("Category")
plt.ylabel("Annotation Count")
plt.xticks(rotation=45)
plt.show()

import numpy as np

# Assuming coco.dataset['images'] is a list of dicts with 'width' and 'height'
widths = [img['width'] for img in coco.dataset['images']]
heights = [img['height'] for img in coco.dataset['images']]

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.hist(widths, bins=30, color='skyblue')
plt.title('Image Width Distribution')
plt.xlabel('Width (pixels)')
plt.ylabel('Count')

plt.subplot(1,2,2)
plt.hist(heights, bins=30, color='lightcoral')
plt.title('Image Height Distribution')
plt.xlabel('Height (pixels)')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

aspect_ratios = []
for ann in coco.dataset['annotations']:
    bbox = ann['bbox']  # [x, y, width, height]
    width, height = bbox[2], bbox[3]
    aspect_ratios.append(width / height if height != 0 else 0)

plt.figure(figsize=(8,5))
plt.hist(aspect_ratios, bins=50, color='purple')
plt.title('Bounding Box Aspect Ratio Distribution (Width / Height)')
plt.xlabel('Aspect Ratio')
plt.ylabel('Count')
plt.show()

img_ids = coco.getImgIds()
ann_counts = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in img_ids]

plt.figure(figsize=(8,5))
sns.histplot(ann_counts, bins=20, kde=False)
plt.title("Number of Annotations per Image")
plt.xlabel("Annotations per Image")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

anns = coco.loadAnns(coco.getAnnIds())
widths = [ann['bbox'][2] for ann in anns]
heights = [ann['bbox'][3] for ann in anns]

plt.figure(figsize=(8,5))
sns.scatterplot(x=widths, y=heights, alpha=0.5)
plt.title("Bounding Box Width vs Height")
plt.xlabel("Width")
plt.ylabel("Height")
plt.tight_layout()
plt.show()

imgs = coco.loadImgs(coco.getImgIds())
img_dims = [(img['width'], img['height']) for img in imgs]
img_df = pd.DataFrame(img_dims, columns=['Width', 'Height'])

plt.figure(figsize=(8,5))
sns.scatterplot(data=img_df, x='Width', y='Height', alpha=0.6)
plt.title("Image Dimension Distribution")
plt.tight_layout()
plt.show()

import numpy as np

anns = coco.loadAnns(coco.getAnnIds())
areas = [ann['area'] for ann in anns]

plt.figure(figsize=(8,5))
sns.histplot(areas, bins=30, kde=True)
plt.title("Distribution of Mask Areas")
plt.xlabel("Area (pixels)")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

img_ids = coco.getImgIds()
cat_ids = coco.getCatIds()

data = []

for img_id in img_ids:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ann_ids)
    counts = {}
    for cat in cat_ids:
        counts[cat] = sum(ann['category_id'] == cat for ann in anns)
    counts['image_id'] = img_id
    data.append(counts)

df = pd.DataFrame(data)
df_melted = df.melt(id_vars=['image_id'], var_name='category_id', value_name='count')
cat_map = {cat['id']: cat['name'] for cat in coco.loadCats(cat_ids)}
df_melted['category'] = df_melted['category_id'].map(cat_map)

plt.figure(figsize=(12,6))
sns.boxplot(x='category', y='count', data=df_melted)
plt.title("Instances per Category per Image")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

anns = coco.loadAnns(coco.getAnnIds())
aspect_ratios = [ann['bbox'][2] / ann['bbox'][3] if ann['bbox'][3] > 0 else 0 for ann in anns]

plt.figure(figsize=(8,5))
sns.histplot(aspect_ratios, bins=40, kde=True)
plt.title("Aspect Ratio Distribution of Bounding Boxes (Width/Height)")
plt.xlabel("Aspect Ratio")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

ann_counts = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in img_ids]
max_idx = ann_counts.index(max(ann_counts))
min_idx = ann_counts.index(min(ann_counts))

max_img_info = coco.loadImgs(img_ids[max_idx])[0]
min_img_info = coco.loadImgs(img_ids[min_idx])[0]

from PIL import Image

def show_image(image_info, title):
    img_path = os.path.join(os.path.dirname(ann_path), 'data', image_info['file_name'])
    img = Image.open(img_path)
    plt.figure(figsize=(8,6))
    plt.imshow(img)
    plt.title(title)
    plt.axis('off')
    plt.show()

show_image(max_img_info, f"Image with Most Annotations ({max(ann_counts)})")
show_image(min_img_info, f"Image with Fewest Annotations ({min(ann_counts)})")

model_save_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth"
torch.save(model.state_dict(), model_save_path)
print("Model saved successfully!")

import torchvision.transforms as T

def get_transform(train):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))
    return T.Compose(transforms)

from PIL import Image
import matplotlib.pyplot as plt

def predict_and_visualize(model, image_path, device, classes, score_threshold=0.5):
    model.eval()
    img = Image.open(image_path).convert("RGB")
    transform = T.ToTensor()
    img_tensor = transform(img).to(device)

    with torch.no_grad():
        output = model([img_tensor])[0]

    plt.figure(figsize=(10, 10))
    plt.imshow(img)
    ax = plt.gca()

    boxes = output['boxes'].cpu()
    labels = output['labels'].cpu()
    scores = output['scores'].cpu()
    masks = output['masks'].cpu()

    for box, label, score, mask in zip(boxes, labels, scores, masks):
        if score < score_threshold:
            continue
        x1, y1, x2, y2 = box
        ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,
                                   fill=False, edgecolor='red', linewidth=2))
        ax.text(x1, y1-10, f"{classes[label]}: {score:.2f}", color='white',
                bbox=dict(facecolor='red', alpha=0.5))
        plt.imshow(mask[0] > 0.5, alpha=0.3)

    plt.axis('off')
    plt.show()

# Example usage:
# predict_and_visualize(model, "/path/to/image.jpg", device, classes)

def save_failure_cases(model, data_loader, device, threshold=0.3, save_dir="./failures"):
    import os
    os.makedirs(save_dir, exist_ok=True)
    model.eval()

    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)

        for i, output in enumerate(outputs):
            scores = output['scores'].cpu().numpy()
            if all(score < threshold for score in scores):
                img = images[i].cpu().permute(1,2,0).numpy()
                plt.imsave(f"{save_dir}/failure_{i}.png", img)

from sklearn.metrics import precision_score, recall_score

def evaluate_per_class_presence(model, data_loader, device, classes, score_threshold=0.5):
    model.eval()
    cat_ids = list(range(len(classes)))

    gt_presence = []
    pred_presence = []

    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)

        for target, output in zip(targets, outputs):
            gt_labels = target['labels'].cpu().numpy()
            pred_labels = output['labels'][output['scores'] > score_threshold].cpu().numpy()

            # For each class, check if present in GT and prediction
            gt_vec = [1 if c in gt_labels else 0 for c in cat_ids]
            pred_vec = [1 if c in pred_labels else 0 for c in cat_ids]

            gt_presence.append(gt_vec)
            pred_presence.append(pred_vec)

    # Convert lists to arrays
    import numpy as np
    gt_presence = np.array(gt_presence)
    pred_presence = np.array(pred_presence)

    print("Per-class Presence Precision & Recall:")
    for idx, cls_name in enumerate(classes):
        prec = precision_score(gt_presence[:, idx], pred_presence[:, idx], zero_division=0)
        rec = recall_score(gt_presence[:, idx], pred_presence[:, idx], zero_division=0)
        print(f"Class {cls_name}: Precision={prec:.2f}, Recall={rec:.2f}")

evaluate_per_class_presence(model, val_loader, device, classes)

import os
from PIL import Image
import matplotlib.pyplot as plt
import torch

mask_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/masks'
mask_files = sorted(os.listdir(mask_dir))

print(f"Found {len(mask_files)} mask files")

# Load first 3 masks as PIL images
masks = [Image.open(os.path.join(mask_dir, f)) for f in mask_files[:3]]
plt.figure(figsize=(12,4))
for i, mask in enumerate(masks):
    plt.subplot(1, 3, i+1)
    plt.imshow(mask, cmap='gray')
    plt.title(f"Mask {i+1}")
    plt.axis('off')
plt.show()

mask_tensors = []
for mask in masks:
    # Convert to tensor, assume mask is grayscale
    tensor = torch.from_numpy(np.array(mask))
    # Binarize: mask pixels > 0 become 1, else 0
    binary_mask = (tensor > 0).to(torch.uint8)
    mask_tensors.append(binary_mask)

print(mask_tensors[0].shape)  # Check shape, should be [H, W]

import torch

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    img_ids = []

    with torch.no_grad():
        for images, targets in data_loader:
            images = list(img.to(device) for img in images)
            outputs = model(images)

            # Assume targets contain 'image_id'
            for output, target in zip(outputs, targets):
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']
                img_ids.append(img_id)

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()
                masks = output.get('masks')
                if masks is not None:
                    masks = masks.cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]  # COCO bbox format: x,y,width,height
                    score = scores[i].item()
                    category_id = labels[i].item()

                    result = {
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    }

                    # Optional: include segmentation if masks are available
                    # You need to encode masks to RLE here if you want segm evaluation

                    results.append(result)
    return results

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()  # set model to eval mode

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    with torch.no_grad():
        for images, targets in data_loader:
            # Move inputs to device
            images = [img.to(device) for img in images]

            outputs = model(images)
            for output, target in zip(outputs, targets):
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]
                    score = scores[i].item()
                    category_id = labels[i].item()

                    result = {
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    }
                    results.append(result)
    return results

predictions = get_coco_predictions(model, val_loader, device)

import json
with open('/content/predictions.json', 'w') as f:
    json.dump(predictions, f)

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    with torch.no_grad():
        for images, targets in data_loader:
            # Move images to device
            images = [img.to(device) for img in images]

            outputs = model(images)

            for output, target in zip(outputs, targets):
                # image_id might be a tensor or int; convert accordingly
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    # Convert [x1, y1, x2, y2] to [x, y, width, height]
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]
                    score = scores[i].item()
                    category_id = labels[i].item()

                    results.append({
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    })
    return results

predictions = get_coco_predictions(model, val_loader, device)

import json
output_path = '/content/predictions.json'
with open(output_path, 'w') as f:
    json.dump(predictions, f)

print(f"Predictions saved to {output_path}")

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# Load ground truth annotations
gt = COCO(val_ann_file)  # your validation annotation JSON path

# Load predictions
preds = gt.loadRes(output_path)

# Initialize COCO evaluator
coco_eval = COCOeval(gt, preds, iouType='bbox')  # or 'segm' if you have masks

coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

import matplotlib.pyplot as plt
import torchvision.transforms.functional as F
import random

def plot_predictions(dataset, predictions, num_images=5):
    for _ in range(num_images):
        idx = random.randint(0, len(dataset)-1)
        img, target = dataset[idx]
        img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

        # Get predictions for this image
        pred_for_img = [p for p in predictions if p['image_id'] == img_id]

        plt.figure(figsize=(10,10))
        plt.imshow(F.to_pil_image(img))

        ax = plt.gca()
        # Plot ground truth boxes (optional)
        for box in target['boxes']:
            xmin, ymin, xmax, ymax = box
            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                 fill=False, edgecolor='green', linewidth=2)
            ax.add_patch(rect)

        # Plot predicted boxes
        for pred in pred_for_img:
            xmin, ymin, w, h = pred['bbox']
            rect = plt.Rectangle((xmin, ymin), w, h,
                                 fill=False, edgecolor='red', linewidth=2)
            ax.add_patch(rect)
            ax.text(xmin, ymin, f"{pred['category_id']}: {pred['score']:.2f}",
                    color='red', fontsize=8, backgroundcolor='white')

        plt.axis('off')
        plt.show()

# Use like this:
plot_predictions(val_dataset, predictions)

model.eval()
predictions = []

for images, targets in val_loader:
    images = list(img.to(device) for img in images)
    with torch.no_grad():
        outputs = model(images)

    for output in outputs:
        # Move outputs to CPU and convert tensors to lists
        pred = {
            'boxes': output['boxes'].cpu().numpy(),
            'labels': output['labels'].cpu().numpy(),
            'scores': output['scores'].cpu().numpy(),
            'masks': output['masks'].cpu().numpy()
        }
        predictions.append(pred)

import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def get_confusion_matrix(predictions, ground_truths, class_names, iou_threshold=0.5):
    """
    Compute and plot confusion matrix for object detection.

    predictions: list of dicts with keys 'boxes', 'labels', 'scores' (all numpy arrays)
    ground_truths: list of dicts with keys 'boxes', 'labels' (all numpy arrays)
    class_names: list of class names
    iou_threshold: threshold to consider a detection a true positive

    Note: This is a simplified matching approach.
    """

    num_classes = len(class_names)
    # Initialize confusion matrix (rows: ground truth, cols: predictions)
    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)

    def iou(box1, box2):
        # box: [x1, y1, x2, y2]
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        inter_area = max(0, x2 - x1) * max(0, y2 - y1)
        box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])
        box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])
        union_area = box1_area + box2_area - inter_area
        return inter_area / union_area if union_area > 0 else 0

    for preds, gts in zip(predictions, ground_truths):
        gt_boxes = gts['boxes']   # numpy array, shape [N,4]
        gt_labels = gts['labels'] # numpy array, shape [N]
        pred_boxes = preds['boxes']  # numpy array, shape [M,4]
        pred_labels = preds['labels']  # numpy array, shape [M]
        pred_scores = preds.get('scores', None)  # optional

        matched_gt = set()
        matched_pred = set()

        # For each prediction, find best matching GT box if IoU > threshold
        for pred_idx, pred_box in enumerate(pred_boxes):
            best_iou = 0
            best_gt_idx = -1
            for gt_idx, gt_box in enumerate(gt_boxes):
                if gt_idx in matched_gt:
                    continue
                iou_val = iou(pred_box, gt_box)
                if iou_val > best_iou:
                    best_iou = iou_val
                    best_gt_idx = gt_idx
            if best_iou >= iou_threshold:
                # Matched detection
                gt_label = gt_labels[best_gt_idx]
                pred_label = pred_labels[pred_idx]
                conf_matrix[gt_label, pred_label] += 1
                matched_gt.add(best_gt_idx)
                matched_pred.add(pred_idx)
            else:
                # False positive (prediction with no matching GT)
                pred_label = pred_labels[pred_idx]
                conf_matrix[-1, pred_label] += 1  # if you want to track FP separately, add a row for background or ignore here

        # False negatives: GT boxes not detected
        for gt_idx, gt_label in enumerate(gt_labels):
            if gt_idx not in matched_gt:
                conf_matrix[gt_label, -1] += 1  # if you want to track FN separately, add a col for background or ignore here

    # Plot confusion matrix heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d',
                xticklabels=class_names + ['FN'],
                yticklabels=class_names + ['FP'])
    plt.xlabel('Predicted')
    plt.ylabel('Ground Truth')
    plt.title('Confusion Matrix for Object Detection')
    plt.show()

    return conf_matrix

import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Flatten all predicted and ground truth labels into 1D arrays
all_preds = []
all_gts = []

for pred, gt in zip(predictions, list_of_ground_truths):
    all_preds.extend(pred['labels'].tolist())
    all_gts.extend(gt['labels'].tolist())

all_preds = np.array(all_preds)
all_gts = np.array(all_gts)

print(f"Total predictions: {len(all_preds)}, Total ground truths: {len(all_gts)}")

def iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    inter_area = max(0, x2-x1) * max(0, y2-y1)
    box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])
    box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])
    union = box1_area + box2_area - inter_area
    return inter_area / union if union > 0 else 0

matched_gt_labels = []
matched_pred_labels = []

iou_threshold = 0.5

for pred, gt in zip(predictions, list_of_ground_truths):
    pred_boxes = pred['boxes']  # numpy arrays, shape [M,4]
    pred_labels = pred['labels']
    gt_boxes = gt['boxes']
    gt_labels = gt['labels']

    matched_gt_indices = set()

    for p_box, p_label in zip(pred_boxes, pred_labels):
        best_iou = 0
        best_gt_idx = -1
        for i, (g_box, g_label) in enumerate(zip(gt_boxes, gt_labels)):
            if i in matched_gt_indices:
                continue
            iou_val = iou(p_box, g_box)
            if iou_val > best_iou:
                best_iou = iou_val
                best_gt_idx = i
        if best_iou >= iou_threshold:
            matched_gt_indices.add(best_gt_idx)
            matched_gt_labels.append(gt_labels[best_gt_idx])
            matched_pred_labels.append(p_label)
        else:
            # False positive - could add to confusion matrix if desired
            pass

# Now plot confusion matrix on these matched labels only
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class_names = ['cake', 'car', 'dog', 'person']

cm = confusion_matrix(matched_gt_labels, matched_pred_labels, labels=range(len(class_names)))

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix (IoU matched)')
plt.show()