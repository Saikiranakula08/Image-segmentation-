# -*- coding: utf-8 -*-
"""Untitled43.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMUUCsMVY5V1awM13bqAvk-_JdYdi4R2
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from pycocotools.coco import COCO
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from google.colab import drive
drive.mount('/content/drive')

train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'

train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300'

with open(train_annotation_file) as f:
    train_coco_data = json.load(f)

print(f"Total training images: {len(train_coco_data['images'])}")
print(f"Total training annotations: {len(train_coco_data['annotations'])}")
print(f"Total categories: {len(train_coco_data['categories'])}")

import json

train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'

train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/data'  # added /data to keep same structure?

# Load training annotation JSON
with open(train_annotation_file) as f:
    train_coco_data = json.load(f)

# Load validation annotation JSON
with open(val_annotation_file) as f:
    val_coco_data = json.load(f)

print(f"Total training images: {len(train_coco_data['images'])}")
print(f"Total training annotations: {len(train_coco_data['annotations'])}")
print(f"Total training categories: {len(train_coco_data['categories'])}")

print(f"Total validation images: {len(val_coco_data['images'])}")
print(f"Total validation annotations: {len(val_coco_data['annotations'])}")
print(f"Total validation categories: {len(val_coco_data['categories'])}")

all_cats = coco_train.loadCats(coco_train.getCatIds())
print("All categories in dataset:")
for c in all_cats:
    print(c['id'], c['name'])

print("Target category IDs:", target_cat_ids)

target_classes = ['cake', 'car', 'dog', 'person']

all_cats = coco_train.loadCats(coco_train.getCatIds())
all_cat_names = [c['name'] for c in all_cats]
print("Dataset categories:", all_cat_names)

# Find closest matches ignoring case
for target in target_classes:
    matches = [c['name'] for c in all_cats if c['name'].lower() == target.lower()]
    print(f"Looking for '{target}' found matches: {matches}")

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)
# Output should be: [15, 16, 25, 41]

anns = coco_train.loadAnns(coco_train.getAnnIds(catIds=target_cat_ids))
print(f"Number of annotations for target categories: {len(anns)}")

img_ids = coco_train.getImgIds(catIds=target_cat_ids)
print(f"Number of images containing target categories: {len(img_ids)}")

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)

image_ids = set(ann['image_id'] for ann in anns)
print(f"Number of images containing target categories (manual): {len(image_ids)}")

for i in range(10):
    ann = coco_train.dataset['annotations'][i]
    cat_id = ann['category_id']
    cat_name = next(cat['name'] for cat in coco_train.dataset['categories'] if cat['id'] == cat_id)
    print(f"Annotation {i} category: {cat_name}")

# Check number of annotations for target categories
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
print(f"Total annotations for target categories: {len(ann_ids)}")

# Load those annotations
anns = coco_train.loadAnns(ann_ids)

# Collect unique image IDs from these annotations
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))
print(f"Number of unique images with target annotations: {len(image_ids_with_targets)}")

# If you have images, let's try visualizing from this list instead
if len(image_ids_with_targets) > 0:
    # Use your visualization function with image_ids_with_targets instead of img_ids
    show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)
else:
    print("No images found with the target categories' annotations.")

image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

show_random_samples(coco_train, image_ids_with_targets, train_img_dir, n=3)

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt

# === Step 1: Paths and COCO setup ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

# Your target classes exactly as per dataset categories
target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)
print("Target category IDs:", target_cat_ids)

# Get all annotations for target categories
ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
print(f"Total annotations for target categories: {len(ann_ids)}")

anns = coco_train.loadAnns(ann_ids)

# Collect unique image IDs with these annotations
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))
print(f"Number of unique images with target annotations: {len(image_ids_with_targets)}")

# === Step 2: Define Albumentations transform ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 3: Custom Dataset ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background
        self.label_to_cat_id = {v: k for k, v in self.cat_id_to_label.items()}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 4: Create dataset and dataloader ===
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# === Step 5: Visualize a batch ===
def visualize_batch(images, masks):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)  # unnormalize
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(10,5))
        plt.subplot(1,2,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,2,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Mask")
        plt.axis('off')

        plt.show()

# Load one batch and visualize
images, masks = next(iter(loader))
visualize_batch(images, masks)

!pip install torchmetrics

import torchmetrics

# --- Prepare validation dataset and loader (no augmentation) ---
val_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json'
val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300'

coco_val = COCO(val_annotation_file)
val_ann_ids = coco_val.getAnnIds(catIds=target_cat_ids)
val_anns = coco_val.loadAnns(val_ann_ids)
val_image_ids = list(set([ann['image_id'] for ann in val_anns]))

val_dataset = CustomCocoDataset(
    coco=coco_val,
    image_ids=val_image_ids,
    img_dir=val_img_dir,
    classes=target_classes,
    transforms=A.Compose([
        A.Resize(256, 256),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])
)

val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

# --- Setup metric ---
metric = torchmetrics.JaccardIndex(num_classes=num_classes).to(device)  # mIoU

# --- Training loop with validation and checkpoint ---
best_val_loss = float('inf')

for epoch in range(epochs):
    # Train
    model.train()
    total_loss = 0
    for images, masks in loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device).long()

        optimizer.zero_grad()
        outputs = model(images)

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    train_loss = total_loss / len(loader)

    # Validate
    model.eval()
    val_loss = 0
    metric.reset()
    with torch.no_grad():
        for images, masks in val_loader:
            images = torch.stack(images).to(device)
            masks = torch.stack(masks).to(device).long()

            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            metric.update(preds, masks)
    val_loss /= len(val_loader)
    val_miou = metric.compute().item()

    print(f"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val mIoU: {val_miou:.4f}")

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_segmentation_model.pth')
        print(f"Saved best model at epoch {epoch+1}")

    # Optional: visualize a sample batch
    sample_images, sample_masks = next(iter(val_loader))
    sample_images_tensor = torch.stack(sample_images).to(device)
    preds = model(sample_images_tensor)
    preds = torch.argmax(preds, dim=1)
    visualize_batch(sample_images, sample_masks, preds=preds)

    model.train()

import os
import random
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
import torch.nn as nn
import torchvision.models as models

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# === Step 1: Setup COCO dataset and paths ===
train_annotation_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
train_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/data'

coco_train = COCO(train_annotation_file)

target_classes = ['cake', 'car', 'dog', 'person']
target_cat_ids = coco_train.getCatIds(catNms=target_classes)

ann_ids = coco_train.getAnnIds(catIds=target_cat_ids)
anns = coco_train.loadAnns(ann_ids)
image_ids_with_targets = list(set([ann['image_id'] for ann in anns]))

print(f"Total images with target classes: {len(image_ids_with_targets)}")
print(f"Total annotations: {len(anns)}")

# === Step 2: Define transforms ===
def get_transform():
    return A.Compose([
        A.Resize(256, 256),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

# === Step 3: Dataset class ===
class CustomCocoDataset(Dataset):
    def __init__(self, coco, image_ids, img_dir, classes, transforms=None):
        self.coco = coco
        self.image_ids = image_ids
        self.img_dir = img_dir
        self.transforms = transforms

        cats = self.coco.loadCats(self.coco.getCatIds(catNms=classes))
        self.cat_id_to_label = {cat['id']: i+1 for i, cat in enumerate(cats)}  # 0=background

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info['file_name'])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=list(self.cat_id_to_label.keys()), iscrowd=None)
        anns = self.coco.loadAnns(ann_ids)

        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)
        for ann in anns:
            cat_id = ann['category_id']
            label = self.cat_id_to_label[cat_id]
            rle = self.coco.annToRLE(ann)
            m = maskUtils.decode(rle)
            mask[m == 1] = label

        if self.transforms:
            augmented = self.transforms(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']
        else:
            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0
            mask = torch.from_numpy(mask).long()

        return image, mask

# === Step 4: Model definition with fixed decoder ===
class SimpleSegmentationModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        self.encoder = nn.Sequential(*list(backbone.children())[:-2])

        self.decoder = nn.Sequential(
            nn.Conv2d(2048, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/32 -> 1/16
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/16 -> 1/8
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/8 -> 1/4
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/4 -> 1/2
            nn.Conv2d(64, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 1/2 -> 1/1 (full res)
            nn.Conv2d(32, num_classes, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# === Step 5: Create dataset and dataloader ===
dataset = CustomCocoDataset(
    coco=coco_train,
    image_ids=image_ids_with_targets,
    img_dir=train_img_dir,
    classes=target_classes,
    transforms=get_transform()
)

def collate_fn(batch):
    return tuple(zip(*batch))

loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

num_classes = len(target_classes) + 1  # background class included
model = SimpleSegmentationModel(num_classes=num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# === Step 6: Training loop with visualization ===
def visualize_batch(images, masks, preds=None):
    batch_size = len(images)
    for i in range(batch_size):
        img = images[i].permute(1,2,0).cpu().numpy()
        img = np.clip(img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)
        mask = masks[i].cpu().numpy()

        plt.figure(figsize=(15,5))
        plt.subplot(1,3,1)
        plt.imshow(img)
        plt.title("Image")
        plt.axis('off')

        plt.subplot(1,3,2)
        plt.imshow(mask, cmap='jet', alpha=0.5)
        plt.title("Ground Truth Mask")
        plt.axis('off')

        if preds is not None:
            pred_mask = preds[i].cpu().numpy()
            plt.subplot(1,3,3)
            plt.imshow(pred_mask, cmap='jet', alpha=0.5)
            plt.title("Predicted Mask")
            plt.axis('off')

        plt.show()

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for images, masks in loader:
        images = torch.stack(images).to(device)
        masks = torch.stack(masks).to(device).long()  # <-- FIX here: cast masks to long

        optimizer.zero_grad()
        outputs = model(images)  # (B, C, H, W)

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.4f}")

    # Visualize predictions on a batch
    model.eval()
    with torch.no_grad():
        sample_images, sample_masks = next(iter(loader))
        sample_images_tensor = torch.stack(sample_images).to(device)
        preds = model(sample_images_tensor)
        preds = torch.argmax(preds, dim=1)  # (B,H,W)
        visualize_batch(sample_images, sample_masks, preds=preds)
    model.train()

label_map = {i: name for i, name in enumerate(target_classes)}
print("Mapped labels:", label_map)

# Get all image IDs that contain at least one target category
img_ids = coco.getImgIds(catIds=target_ids)
print(f"Number of images with target classes: {len(img_ids)}")

import numpy as np
from pycocotools import mask as maskUtils

def load_mask(image_id, target_ids, label_map, img_shape):
    ann_ids = coco.getAnnIds(imgIds=image_id, catIds=target_ids, iscrowd=None)
    anns = coco.loadAnns(ann_ids)

    mask = np.zeros(img_shape[:2], dtype=np.uint8)  # single channel mask, default background=0

    for ann in anns:
        cat_id = ann['category_id']
        class_idx = list(label_map.keys()).index(cat_id) + 1  # 1-based class index to leave 0 for background
        # Convert COCO RLE or polygon to binary mask
        rle = coco.annToRLE(ann)
        m = maskUtils.decode(rle)
        mask[m == 1] = class_idx

    return mask

import cv2

def load_image(image_id, img_dir):
    img_info = coco.loadImgs(image_id)[0]
    img_path = f"{img_dir}/{img_info['file_name']}"
    image = cv2.imread(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
    return image

from pycocotools.coco import COCO

# Load COCO annotations
ann_file = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json'
coco = COCO(ann_file)

# Define your exact target classes
target_classes = ['cake', 'car', 'dog', 'person']

# Get category IDs for your target classes only
target_ids = coco.getCatIds(catNms=target_classes)

# Load category info for these IDs
target_cats = coco.loadCats(target_ids)

# Create a label map: category ID → category name
label_map = {cat['id']: cat['name'] for cat in target_cats}

print("Filtered label_map:", label_map)

import torchvision.transforms as T

def get_transform(train=False):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transforms)

train_dataset = CustomCocoDataset(
    train_img_dir,
    train_ann_file,
    classes[1:],
    transforms=get_transform(train=True)
)

val_dataset = CustomCocoDataset(
    val_img_dir,
    val_ann_file,
    classes[1:],
    transforms=get_transform(train=False)
)

import torch
from torch.utils.data import Dataset
from PIL import Image
from pycocotools.coco import COCO
import numpy as np
import os

class CustomCocoDataset(Dataset):
    def __init__(self, img_dir, ann_file, classes, transforms=None):
        self.img_dir = img_dir
        self.coco = COCO(ann_file)
        self.ids = list(sorted(self.coco.imgs.keys()))
        self.classes = classes
        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}
        self.transforms = transforms

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, index):
        img_id = self.ids[index]
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)
        img_info = self.coco.loadImgs(img_id)[0]

        img_path = os.path.join(self.img_dir, img_info['file_name'])
        img = Image.open(img_path).convert("RGB")

        masks = []
        boxes = []
        labels = []
        areas = []
        iscrowd = []

        for ann in anns:
            cat_name = self.coco.loadCats(ann['category_id'])[0]['name']
            if cat_name not in self.classes:
                continue
            mask = self.coco.annToMask(ann)
            masks.append(mask)
            x, y, w, h = ann['bbox']
            boxes.append([x, y, x + w, y + h])
            labels.append(self.class_to_idx[cat_name])
            areas.append(ann['area'])
            iscrowd.append(ann.get('iscrowd', 0))

        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            masks = torch.zeros((0, img.height, img.width), dtype=torch.uint8)
            labels = torch.zeros((0,), dtype=torch.int64)
            areas = torch.zeros((0,), dtype=torch.float32)
            iscrowd = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            masks = torch.as_tensor(np.stack(masks, axis=0), dtype=torch.uint8)
            labels = torch.as_tensor(labels, dtype=torch.int64)
            areas = torch.as_tensor(areas, dtype=torch.float32)
            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "masks": masks,
            "image_id": torch.tensor([img_id]),
            "area": areas,
            "iscrowd": iscrowd,
        }

        if self.transforms:
            img, target = self.transforms(img, target)

        return img, target

from torch.utils.data import DataLoader

def collate_fn(batch):
    return tuple(zip(*batch))

train_dataset = CustomCocoDataset(train_img_dir, train_ann_file, classes[1:], transforms=get_transform(train=True))
val_dataset = CustomCocoDataset(val_img_dir, val_ann_file, classes[1:], transforms=get_transform(train=False))

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
import torch

num_classes = len(classes)  # background + your classes

model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

# Replace box predictor
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

# Replace mask predictor
in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
hidden_layer = 256
model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):
    model.train()
    running_loss = 0.0

    for i, (images, targets) in enumerate(data_loader):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        running_loss += losses.item()

        if i % print_freq == 0:
            print(f"Epoch [{epoch}], Step [{i}/{len(data_loader)}], Loss: {losses.item():.4f}")

    avg_loss = running_loss / len(data_loader)
    print(f"Epoch [{epoch}] completed. Average Loss: {avg_loss:.4f}")
    return avg_loss

num_epochs = 5
for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)
    lr_scheduler.step()

label_map = {
    16: 'cake',
    17: 'car',
    26: 'dog',
    41: 'person'
}

import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
import numpy as np

def visualize_predictions_and_targets(model, data_loader, device, class_names, num_images=3, score_threshold=0.5):
    model.eval()
    count = 0
    for images, targets in data_loader:
        images = list(img.to(device) for img in images)
        outputs = model(images)

        for i in range(len(images)):
            img = images[i].cpu()
            target = targets[i]
            output = outputs[i]

            boxes_pred = output['boxes'].cpu().detach().numpy()
            labels_pred = output['labels'].cpu().detach().numpy()
            scores_pred = output['scores'].cpu().detach().numpy()
            masks_pred = output['masks'].cpu().detach().numpy()

            boxes_gt = target['boxes'].cpu().numpy()
            labels_gt = target['labels'].cpu().numpy()

            keep = scores_pred >= score_threshold

            img_pil = TF.to_pil_image(img)
            plt.figure(figsize=(16, 8))

            # Plot ground truth on the left
            plt.subplot(1, 2, 1)
            plt.imshow(img_pil)
            ax = plt.gca()
            plt.title("Ground Truth")

            for box, label in zip(boxes_gt, labels_gt):
                x1, y1, x2, y2 = box
                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                     fill=False, edgecolor='green', linewidth=2)
                ax.add_patch(rect)
                ax.text(x1, y1 - 10, class_names[label - 1] if label > 0 else 'N/A',
                        color='green', fontsize=10, backgroundcolor='black')

            plt.axis('off')

            # Plot predictions on the right
            plt.subplot(1, 2, 2)
            plt.imshow(img_pil)
            ax = plt.gca()
            plt.title("Predictions")

            for box, label, score, mask in zip(boxes_pred[keep], labels_pred[keep], scores_pred[keep], masks_pred[keep]):
                x1, y1, x2, y2 = box
                rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                     fill=False, edgecolor='red', linewidth=2)
                ax.add_patch(rect)

                label_name = class_names[label - 1] if label > 0 else 'N/A'
                ax.text(x1, y1 - 10, f"{label_name}: {score:.2f}",
                        color='red', fontsize=10, backgroundcolor='black')

                mask_bin = mask[0] > 0.5
                plt.imshow(mask_bin, alpha=0.3)

            plt.axis('off')

            plt.show()

            count += 1
            if count >= num_images:
                return

class_names = ['car', 'cake', 'dog', 'person']  # your classes list
visualize_predictions_and_targets(model, val_loader, device, class_names, num_images=9, score_threshold=0.5)

model.eval()
for images, targets in val_loader:
    images = list(img.to(device) for img in images)
    outputs = model(images)

    for i in range(len(images)):
        output = outputs[i]

        boxes = output['boxes'].cpu().detach().numpy()
        labels = output['labels'].cpu().detach().numpy()
        scores = output['scores'].cpu().detach().numpy()

        print(f"Image {i+1} predictions:")
        for box, label, score in zip(boxes, labels, scores):
            print(f"  Label: {label}, Score: {score:.3f}, Box: {box}")

    break  # remove break if you want to check more batches

for i in range(len(images)):
    output = outputs[i]
    target = targets[i]

    print(f"Image {i+1} ground truth:")
    for box, label in zip(target['boxes'], target['labels']):
        print(f"  Label: {label.item()}, Box: {box.tolist()}")

    print(f"Image {i+1} predictions:")
    boxes = output['boxes'].cpu().detach().numpy()
    labels = output['labels'].cpu().detach().numpy()
    scores = output['scores'].cpu().detach().numpy()
    for box, label, score in zip(boxes, labels, scores):
        print(f"  Label: {label}, Score: {score:.3f}, Box: {box}")

model_save_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth"
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")

model.eval()

import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF

def show_predictions(model, data_loader, device, class_names, num_images=10, score_threshold=0.5):
    model.eval()
    count = 0
    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)

        for i in range(len(images)):
            img = images[i].cpu()
            output = outputs[i]
            boxes = output['boxes'].cpu().detach().numpy()
            scores = output['scores'].cpu().detach().numpy()
            labels = output['labels'].cpu().detach().numpy()
            masks = output['masks'].cpu().detach().numpy()

            keep = scores >= score_threshold
            plt.figure(figsize=(8, 8))
            plt.imshow(TF.to_pil_image(img))
            ax = plt.gca()

            for box, label, score, mask in zip(boxes[keep], labels[keep], scores[keep], masks[keep]):
                x1, y1, x2, y2 = box
                ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                           fill=False, color='lime', linewidth=2))
                ax.text(x1, y1 - 5, f"{class_names[label]}: {score:.2f}",
                        color='white', backgroundcolor='green', fontsize=9)
                plt.imshow(mask[0] > 0.5, alpha=0.3)

            plt.axis("off")
            plt.show()

            count += 1
            if count >= num_images:
                return

show_predictions(model, val_loader, device, class_names=classes)

from pycocotools.coco import COCO
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os

ann_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/labels.json"
coco = COCO(ann_path)

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you already have:
# cat_names = list of category names
# cat_counts = number of annotations per category

plt.figure(figsize=(10,6))
sns.barplot(x=cat_names, y=cat_counts, palette='viridis')
plt.title("Number of Annotations per Category")
plt.xlabel("Category")
plt.ylabel("Annotation Count")
plt.xticks(rotation=45)
plt.show()

import numpy as np

# Assuming coco.dataset['images'] is a list of dicts with 'width' and 'height'
widths = [img['width'] for img in coco.dataset['images']]
heights = [img['height'] for img in coco.dataset['images']]

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.hist(widths, bins=30, color='skyblue')
plt.title('Image Width Distribution')
plt.xlabel('Width (pixels)')
plt.ylabel('Count')

plt.subplot(1,2,2)
plt.hist(heights, bins=30, color='lightcoral')
plt.title('Image Height Distribution')
plt.xlabel('Height (pixels)')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Get number of annotations per image
from collections import Counter

image_ids = [ann['image_id'] for ann in coco.dataset['annotations']]
ann_per_image = Counter(image_ids)
ann_counts = list(ann_per_image.values())

plt.figure(figsize=(8,5))
plt.hist(ann_counts, bins=range(1, max(ann_counts)+1), color='mediumseagreen', align='left')
plt.title('Number of Annotations per Image')
plt.xlabel('Annotations Count')
plt.ylabel('Number of Images')
plt.show()

aspect_ratios = []
for ann in coco.dataset['annotations']:
    bbox = ann['bbox']  # [x, y, width, height]
    width, height = bbox[2], bbox[3]
    aspect_ratios.append(width / height if height != 0 else 0)

plt.figure(figsize=(8,5))
plt.hist(aspect_ratios, bins=50, color='purple')
plt.title('Bounding Box Aspect Ratio Distribution (Width / Height)')
plt.xlabel('Aspect Ratio')
plt.ylabel('Count')
plt.show()

cats = coco.loadCats(coco.getCatIds())
cat_names = [cat['name'] for cat in cats]
cat_ids = coco.getCatIds()
cat_counts = [len(coco.getAnnIds(catIds=[cat_id])) for cat_id in cat_ids]

plt.figure(figsize=(8,5))
sns.barplot(x=cat_names, y=cat_counts, palette="viridis")
plt.title("Number of Annotations per Category")
plt.ylabel("Count")
plt.xlabel("Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

img_ids = coco.getImgIds()
ann_counts = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in img_ids]

plt.figure(figsize=(8,5))
sns.histplot(ann_counts, bins=20, kde=False)
plt.title("Number of Annotations per Image")
plt.xlabel("Annotations per Image")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

anns = coco.loadAnns(coco.getAnnIds())
widths = [ann['bbox'][2] for ann in anns]
heights = [ann['bbox'][3] for ann in anns]

plt.figure(figsize=(8,5))
sns.scatterplot(x=widths, y=heights, alpha=0.5)
plt.title("Bounding Box Width vs Height")
plt.xlabel("Width")
plt.ylabel("Height")
plt.tight_layout()
plt.show()

imgs = coco.loadImgs(coco.getImgIds())
img_dims = [(img['width'], img['height']) for img in imgs]
img_df = pd.DataFrame(img_dims, columns=['Width', 'Height'])

plt.figure(figsize=(8,5))
sns.scatterplot(data=img_df, x='Width', y='Height', alpha=0.6)
plt.title("Image Dimension Distribution")
plt.tight_layout()
plt.show()

import numpy as np

anns = coco.loadAnns(coco.getAnnIds())
areas = [ann['area'] for ann in anns]

plt.figure(figsize=(8,5))
sns.histplot(areas, bins=30, kde=True)
plt.title("Distribution of Mask Areas")
plt.xlabel("Area (pixels)")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

img_ids = coco.getImgIds()
cat_ids = coco.getCatIds()

data = []

for img_id in img_ids:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ann_ids)
    counts = {}
    for cat in cat_ids:
        counts[cat] = sum(ann['category_id'] == cat for ann in anns)
    counts['image_id'] = img_id
    data.append(counts)

df = pd.DataFrame(data)
df_melted = df.melt(id_vars=['image_id'], var_name='category_id', value_name='count')
cat_map = {cat['id']: cat['name'] for cat in coco.loadCats(cat_ids)}
df_melted['category'] = df_melted['category_id'].map(cat_map)

plt.figure(figsize=(12,6))
sns.boxplot(x='category', y='count', data=df_melted)
plt.title("Instances per Category per Image")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

anns = coco.loadAnns(coco.getAnnIds())
aspect_ratios = [ann['bbox'][2] / ann['bbox'][3] if ann['bbox'][3] > 0 else 0 for ann in anns]

plt.figure(figsize=(8,5))
sns.histplot(aspect_ratios, bins=40, kde=True)
plt.title("Aspect Ratio Distribution of Bounding Boxes (Width/Height)")
plt.xlabel("Aspect Ratio")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

cat_ids = coco.getCatIds()
cat_img_counts = []

for cat_id in cat_ids:
    img_ids = coco.getImgIds(catIds=[cat_id])
    cat_img_counts.append(len(img_ids))

cat_names = [cat['name'] for cat in coco.loadCats(cat_ids)]

plt.figure(figsize=(8,5))
sns.barplot(x=cat_names, y=cat_img_counts, palette="coolwarm")
plt.title("Number of Images per Category")
plt.ylabel("Number of Images")
plt.xlabel("Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

ann_counts = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in img_ids]
max_idx = ann_counts.index(max(ann_counts))
min_idx = ann_counts.index(min(ann_counts))

max_img_info = coco.loadImgs(img_ids[max_idx])[0]
min_img_info = coco.loadImgs(img_ids[min_idx])[0]

from PIL import Image

def show_image(image_info, title):
    img_path = os.path.join(os.path.dirname(ann_path), 'data', image_info['file_name'])
    img = Image.open(img_path)
    plt.figure(figsize=(8,6))
    plt.imshow(img)
    plt.title(title)
    plt.axis('off')
    plt.show()

show_image(max_img_info, f"Image with Most Annotations ({max(ann_counts)})")
show_image(min_img_info, f"Image with Fewest Annotations ({min(ann_counts)})")

model_save_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth"
torch.save(model.state_dict(), model_save_path)
print("Model saved successfully!")

import torchvision.transforms as T

def get_transform(train):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
        transforms.append(T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))
    return T.Compose(transforms)

from PIL import Image
import matplotlib.pyplot as plt

def predict_and_visualize(model, image_path, device, classes, score_threshold=0.5):
    model.eval()
    img = Image.open(image_path).convert("RGB")
    transform = T.ToTensor()
    img_tensor = transform(img).to(device)

    with torch.no_grad():
        output = model([img_tensor])[0]

    plt.figure(figsize=(10, 10))
    plt.imshow(img)
    ax = plt.gca()

    boxes = output['boxes'].cpu()
    labels = output['labels'].cpu()
    scores = output['scores'].cpu()
    masks = output['masks'].cpu()

    for box, label, score, mask in zip(boxes, labels, scores, masks):
        if score < score_threshold:
            continue
        x1, y1, x2, y2 = box
        ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,
                                   fill=False, edgecolor='red', linewidth=2))
        ax.text(x1, y1-10, f"{classes[label]}: {score:.2f}", color='white',
                bbox=dict(facecolor='red', alpha=0.5))
        plt.imshow(mask[0] > 0.5, alpha=0.3)

    plt.axis('off')
    plt.show()

# Example usage:
# predict_and_visualize(model, "/path/to/image.jpg", device, classes)

def save_failure_cases(model, data_loader, device, threshold=0.3, save_dir="./failures"):
    import os
    os.makedirs(save_dir, exist_ok=True)
    model.eval()

    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)

        for i, output in enumerate(outputs):
            scores = output['scores'].cpu().numpy()
            if all(score < threshold for score in scores):
                img = images[i].cpu().permute(1,2,0).numpy()
                plt.imsave(f"{save_dir}/failure_{i}.png", img)

from sklearn.metrics import precision_score, recall_score

def evaluate_per_class_presence(model, data_loader, device, classes, score_threshold=0.5):
    model.eval()
    cat_ids = list(range(len(classes)))

    gt_presence = []
    pred_presence = []

    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)

        for target, output in zip(targets, outputs):
            gt_labels = target['labels'].cpu().numpy()
            pred_labels = output['labels'][output['scores'] > score_threshold].cpu().numpy()

            # For each class, check if present in GT and prediction
            gt_vec = [1 if c in gt_labels else 0 for c in cat_ids]
            pred_vec = [1 if c in pred_labels else 0 for c in cat_ids]

            gt_presence.append(gt_vec)
            pred_presence.append(pred_vec)

    # Convert lists to arrays
    import numpy as np
    gt_presence = np.array(gt_presence)
    pred_presence = np.array(pred_presence)

    print("Per-class Presence Precision & Recall:")
    for idx, cls_name in enumerate(classes):
        prec = precision_score(gt_presence[:, idx], pred_presence[:, idx], zero_division=0)
        rec = recall_score(gt_presence[:, idx], pred_presence[:, idx], zero_division=0)
        print(f"Class {cls_name}: Precision={prec:.2f}, Recall={rec:.2f}")

evaluate_per_class_presence(model, val_loader, device, classes)

import os
from PIL import Image
import matplotlib.pyplot as plt
import torch

mask_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300/masks'
mask_files = sorted(os.listdir(mask_dir))

print(f"Found {len(mask_files)} mask files")

# Load first 3 masks as PIL images
masks = [Image.open(os.path.join(mask_dir, f)) for f in mask_files[:3]]
plt.figure(figsize=(12,4))
for i, mask in enumerate(masks):
    plt.subplot(1, 3, i+1)
    plt.imshow(mask, cmap='gray')
    plt.title(f"Mask {i+1}")
    plt.axis('off')
plt.show()

mask_tensors = []
for mask in masks:
    # Convert to tensor, assume mask is grayscale
    tensor = torch.from_numpy(np.array(mask))
    # Binarize: mask pixels > 0 become 1, else 0
    binary_mask = (tensor > 0).to(torch.uint8)
    mask_tensors.append(binary_mask)

print(mask_tensors[0].shape)  # Check shape, should be [H, W]

import os
from PIL import Image
import matplotlib.pyplot as plt

val_img_dir = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/data'
val_img_files = sorted(os.listdir(val_img_dir))

print(f"Found {len(val_img_files)} validation images")

# Load first 3 images
images = [Image.open(os.path.join(val_img_dir, f)) for f in val_img_files[:3]]

plt.figure(figsize=(12,4))
for i, img in enumerate(images):
    plt.subplot(1,3,i+1)
    plt.imshow(img)
    plt.title(f"Validation image {i+1}")
    plt.axis('off')
plt.show()

import torch

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    img_ids = []

    with torch.no_grad():
        for images, targets in data_loader:
            images = list(img.to(device) for img in images)
            outputs = model(images)

            # Assume targets contain 'image_id'
            for output, target in zip(outputs, targets):
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']
                img_ids.append(img_id)

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()
                masks = output.get('masks')
                if masks is not None:
                    masks = masks.cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]  # COCO bbox format: x,y,width,height
                    score = scores[i].item()
                    category_id = labels[i].item()

                    result = {
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    }

                    # Optional: include segmentation if masks are available
                    # You need to encode masks to RLE here if you want segm evaluation

                    results.append(result)
    return results

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()  # set model to eval mode

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    with torch.no_grad():
        for images, targets in data_loader:
            # Move inputs to device
            images = [img.to(device) for img in images]

            outputs = model(images)
            for output, target in zip(outputs, targets):
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]
                    score = scores[i].item()
                    category_id = labels[i].item()

                    result = {
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    }
                    results.append(result)
    return results

predictions = get_coco_predictions(model, val_loader, device)

import json
with open('/content/predictions.json', 'w') as f:
    json.dump(predictions, f)

import torch

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()

def get_coco_predictions(model, data_loader, device):
    model.eval()
    results = []
    with torch.no_grad():
        for images, targets in data_loader:
            # Move images to device
            images = [img.to(device) for img in images]

            outputs = model(images)

            for output, target in zip(outputs, targets):
                # image_id might be a tensor or int; convert accordingly
                img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

                boxes = output['boxes'].cpu()
                scores = output['scores'].cpu()
                labels = output['labels'].cpu()

                for i in range(len(boxes)):
                    box = boxes[i].tolist()
                    # Convert [x1, y1, x2, y2] to [x, y, width, height]
                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]
                    score = scores[i].item()
                    category_id = labels[i].item()

                    results.append({
                        "image_id": img_id,
                        "category_id": category_id,
                        "bbox": bbox,
                        "score": score
                    })
    return results

predictions = get_coco_predictions(model, val_loader, device)

import json
output_path = '/content/predictions.json'
with open(output_path, 'w') as f:
    json.dump(predictions, f)

print(f"Predictions saved to {output_path}")

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# Load ground truth annotations
gt = COCO(val_ann_file)  # your validation annotation JSON path

# Load predictions
preds = gt.loadRes(output_path)

# Initialize COCO evaluator
coco_eval = COCOeval(gt, preds, iouType='bbox')  # or 'segm' if you have masks

coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

import matplotlib.pyplot as plt
import torchvision.transforms.functional as F
import random

def plot_predictions(dataset, predictions, num_images=5):
    for _ in range(num_images):
        idx = random.randint(0, len(dataset)-1)
        img, target = dataset[idx]
        img_id = target['image_id'].item() if isinstance(target['image_id'], torch.Tensor) else target['image_id']

        # Get predictions for this image
        pred_for_img = [p for p in predictions if p['image_id'] == img_id]

        plt.figure(figsize=(10,10))
        plt.imshow(F.to_pil_image(img))

        ax = plt.gca()
        # Plot ground truth boxes (optional)
        for box in target['boxes']:
            xmin, ymin, xmax, ymax = box
            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                 fill=False, edgecolor='green', linewidth=2)
            ax.add_patch(rect)

        # Plot predicted boxes
        for pred in pred_for_img:
            xmin, ymin, w, h = pred['bbox']
            rect = plt.Rectangle((xmin, ymin), w, h,
                                 fill=False, edgecolor='red', linewidth=2)
            ax.add_patch(rect)
            ax.text(xmin, ymin, f"{pred['category_id']}: {pred['score']:.2f}",
                    color='red', fontsize=8, backgroundcolor='white')

        plt.axis('off')
        plt.show()

# Use like this:
plot_predictions(val_dataset, predictions)

model.eval()
predictions = []

for images, targets in val_loader:
    images = list(img.to(device) for img in images)
    with torch.no_grad():
        outputs = model(images)

    for output in outputs:
        # Move outputs to CPU and convert tensors to lists
        pred = {
            'boxes': output['boxes'].cpu().numpy(),
            'labels': output['labels'].cpu().numpy(),
            'scores': output['scores'].cpu().numpy(),
            'masks': output['masks'].cpu().numpy()
        }
        predictions.append(pred)

import matplotlib.pyplot as plt
import numpy as np

def plot_predictions(image, pred, threshold=0.5):
    plt.figure(figsize=(10,10))
    plt.imshow(image.permute(1, 2, 0).cpu())  # Convert tensor to HWC image

    masks = pred['masks']
    scores = pred['scores']

    for i in range(len(scores)):
        if scores[i] > threshold:
            mask = masks[i, 0] > 0.5  # binary mask
            plt.imshow(np.ma.masked_where(~mask, mask), alpha=0.5)  # Overlay mask

    plt.axis('off')
    plt.show()

# Example on first val image
images, targets = next(iter(val_loader))
plot_predictions(images[1], predictions[1])

list_of_ground_truths = []
for _, target in val_dataset:
    # target is a dict with tensors, convert tensors to CPU numpy for convenience
    gt = {
        'boxes': target['boxes'].cpu().numpy(),
        'labels': target['labels'].cpu().numpy()
    }
    list_of_ground_truths.append(gt)
model.eval()
predictions = []
with torch.no_grad():
    for images, _ in val_loader:
        images = list(img.to(device) for img in images)
        outputs = model(images)
        # Convert outputs to CPU numpy
        for output in outputs:
            pred = {
                'boxes': output['boxes'].cpu(),
                'labels': output['labels'].cpu(),
                'scores': output['scores'].cpu()
            }
            predictions.append(pred)

import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def get_confusion_matrix(predictions, ground_truths, class_names, iou_threshold=0.5):
    """
    Compute and plot confusion matrix for object detection.

    predictions: list of dicts with keys 'boxes', 'labels', 'scores' (all numpy arrays)
    ground_truths: list of dicts with keys 'boxes', 'labels' (all numpy arrays)
    class_names: list of class names
    iou_threshold: threshold to consider a detection a true positive

    Note: This is a simplified matching approach.
    """

    num_classes = len(class_names)
    # Initialize confusion matrix (rows: ground truth, cols: predictions)
    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)

    def iou(box1, box2):
        # box: [x1, y1, x2, y2]
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        inter_area = max(0, x2 - x1) * max(0, y2 - y1)
        box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])
        box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])
        union_area = box1_area + box2_area - inter_area
        return inter_area / union_area if union_area > 0 else 0

    for preds, gts in zip(predictions, ground_truths):
        gt_boxes = gts['boxes']   # numpy array, shape [N,4]
        gt_labels = gts['labels'] # numpy array, shape [N]
        pred_boxes = preds['boxes']  # numpy array, shape [M,4]
        pred_labels = preds['labels']  # numpy array, shape [M]
        pred_scores = preds.get('scores', None)  # optional

        matched_gt = set()
        matched_pred = set()

        # For each prediction, find best matching GT box if IoU > threshold
        for pred_idx, pred_box in enumerate(pred_boxes):
            best_iou = 0
            best_gt_idx = -1
            for gt_idx, gt_box in enumerate(gt_boxes):
                if gt_idx in matched_gt:
                    continue
                iou_val = iou(pred_box, gt_box)
                if iou_val > best_iou:
                    best_iou = iou_val
                    best_gt_idx = gt_idx
            if best_iou >= iou_threshold:
                # Matched detection
                gt_label = gt_labels[best_gt_idx]
                pred_label = pred_labels[pred_idx]
                conf_matrix[gt_label, pred_label] += 1
                matched_gt.add(best_gt_idx)
                matched_pred.add(pred_idx)
            else:
                # False positive (prediction with no matching GT)
                pred_label = pred_labels[pred_idx]
                conf_matrix[-1, pred_label] += 1  # if you want to track FP separately, add a row for background or ignore here

        # False negatives: GT boxes not detected
        for gt_idx, gt_label in enumerate(gt_labels):
            if gt_idx not in matched_gt:
                conf_matrix[gt_label, -1] += 1  # if you want to track FN separately, add a col for background or ignore here

    # Plot confusion matrix heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d',
                xticklabels=class_names + ['FN'],
                yticklabels=class_names + ['FP'])
    plt.xlabel('Predicted')
    plt.ylabel('Ground Truth')
    plt.title('Confusion Matrix for Object Detection')
    plt.show()

    return conf_matrix

import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Flatten all predicted and ground truth labels into 1D arrays
all_preds = []
all_gts = []

for pred, gt in zip(predictions, list_of_ground_truths):
    all_preds.extend(pred['labels'].tolist())
    all_gts.extend(gt['labels'].tolist())

all_preds = np.array(all_preds)
all_gts = np.array(all_gts)

print(f"Total predictions: {len(all_preds)}, Total ground truths: {len(all_gts)}")

def iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    inter_area = max(0, x2-x1) * max(0, y2-y1)
    box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])
    box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])
    union = box1_area + box2_area - inter_area
    return inter_area / union if union > 0 else 0

matched_gt_labels = []
matched_pred_labels = []

iou_threshold = 0.5

for pred, gt in zip(predictions, list_of_ground_truths):
    pred_boxes = pred['boxes']  # numpy arrays, shape [M,4]
    pred_labels = pred['labels']
    gt_boxes = gt['boxes']
    gt_labels = gt['labels']

    matched_gt_indices = set()

    for p_box, p_label in zip(pred_boxes, pred_labels):
        best_iou = 0
        best_gt_idx = -1
        for i, (g_box, g_label) in enumerate(zip(gt_boxes, gt_labels)):
            if i in matched_gt_indices:
                continue
            iou_val = iou(p_box, g_box)
            if iou_val > best_iou:
                best_iou = iou_val
                best_gt_idx = i
        if best_iou >= iou_threshold:
            matched_gt_indices.add(best_gt_idx)
            matched_gt_labels.append(gt_labels[best_gt_idx])
            matched_pred_labels.append(p_label)
        else:
            # False positive - could add to confusion matrix if desired
            pass

# Now plot confusion matrix on these matched labels only
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class_names = ['cake', 'car', 'dog', 'person']

cm = confusion_matrix(matched_gt_labels, matched_pred_labels, labels=range(len(class_names)))

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix (IoU matched)')
plt.show()

import torchvision
from torchvision.models.detection import maskrcnn_resnet50_fpn

num_classes = 5  # 4 classes + background

# Load model
model = maskrcnn_resnet50_fpn(pretrained=False)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
hidden_layer = 256
model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

# Load trained weights if you have a checkpoint
checkpoint_path = '/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/model_final.pth'
model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.eval()

# Class list - background is index 0, then your 4 classes
classes = ['__background__', 'cake', 'car', 'dog', 'person']

# Create label map from class index to name (skip background=0)
label_map = {i: c for i, c in enumerate(classes) if i != 0}

from torch.utils.data import DataLoader

val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

import matplotlib.pyplot as plt
import cv2
import torchvision.transforms.functional as F

def visualize_predictions(model, data_loader, device, label_map, num_images=5, score_threshold=0.5):
    model.eval()
    count = 0

    for images, targets in data_loader:
        images = list(img.to(device) for img in images)

        with torch.no_grad():
            outputs = model(images)

        for i in range(len(images)):
            img_tensor = images[i].cpu()
            output = outputs[i]

            boxes = output['boxes'].cpu().numpy()
            labels = output['labels'].cpu().numpy()
            scores = output['scores'].cpu().numpy()

            # Convert tensor image to numpy for OpenCV and matplotlib
            img_np = img_tensor.mul(255).permute(1, 2, 0).byte().numpy()
            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

            for box, label, score in zip(boxes, labels, scores):
                if score < score_threshold:
                    continue
                if label not in label_map:
                    continue
                box = box.astype(int)
                color = (0, 255, 0)  # green box
                cv2.rectangle(img_cv, (box[0], box[1]), (box[2], box[3]), color, 2)
                text = f"{label_map[label]}: {score:.2f}"
                cv2.putText(img_cv, text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)
            plt.figure(figsize=(10, 10))
            plt.imshow(img_rgb)
            plt.axis('off')
            plt.title(f"Prediction {count+1}")
            plt.show()

            count += 1
            if count >= num_images:
                return

# Map your original COCO category IDs to model label IDs (1 to 4)
original_to_model_label = {
    1: 1,    # person
    17: 2,   # cat
    18: 3,   # dog
    3: 4     # car
}

visualize_predictions(model, val_loader, device, label_map, num_images=5, score_threshold=0.5)

from pycocotools.coco import COCO

coco = COCO('/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300/labels.json')
cats = coco.loadCats(coco.getCatIds())

# Full mapping from COCO
full_label_map = {cat['id']: cat['name'] for cat in cats}

# Define the 4 classes you want to keep
wanted_classes = ['car', 'cake', 'dog', 'person']

# Filter to keep only those classes
label_map = {cat_id: name for cat_id, name in full_label_map.items() if name in wanted_classes}

print(label_map)

import matplotlib.pyplot as plt

# Count annotations per category
from collections import Counter

labels = [category_id_to_label[ann['category_id']] for ann in filtered_annotations]
label_counts = Counter(labels)

# Reverse mapping label_id -> class name
label_id_to_name = {v:k for k,v in category_name_to_id.items()}

class_names = [label_id_to_name[label] for label in sorted(label_counts.keys())]
counts = [label_counts[label] for label in sorted(label_counts.keys())]

plt.figure(figsize=(8,5))
plt.bar(class_names, counts, color='skyblue')
plt.title("Number of Annotations per Class")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()